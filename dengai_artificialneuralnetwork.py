# -*- coding: utf-8 -*-
"""DengAI_ArtificialNeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/0B23lPPhL7HcsSG90bEZIemhHNXNFWVdua3NHMnBZdHd6T0ZB
"""

# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras  import regularizers

from sklearn import metrics

"""# Loading and Ploting data
1. Plot 1, weekofyear over labels, at the week 20 goes bottom and the week 42 goes top -> seasonal factor
2. Plot 2, week_start_date over labels, some spikes toward the end of years 1994 and 1998 -> outlier
"""

df=pd.read_csv('https://s3.amazonaws.com/drivendata/data/44/public/dengue_features_train.csv')
labels=pd.read_csv('https://s3.amazonaws.com/drivendata/data/44/public/dengue_labels_train.csv')
#print(labels.tail())
df['label']=labels['total_cases']
df['city']=labels['city']
df['week_start_date']=pd.to_datetime(df.week_start_date)
df=df.sort_values('week_start_date',ascending=True)
df.plot(kind='scatter',x='weekofyear',y='label',color='red')
df.plot('week_start_date','label',color='red')
plt.show()

"""# Creating dummy variables 
1. adding label features of previous two weeks (2-pweeks)
2. adding seasonal features
3. adding outlier features
"""

dfiq=df[df.city!='sj']
dfsj=df[df.city!='iq']

 
dfiq['pre_1_week_label']=dfiq['label'].shift(-1)
dfiq['pre_2_week_label']=dfiq['label'].shift(-2)
#dfiq['pre_3_week_label']=dfiq['label'].shift(-3)
dfsj['pre_1_week_label']=dfsj['label'].shift(-1)
dfsj['pre_2_week_label']=dfsj['label'].shift(-2)
#dfsj['pre_3_week_label']=dfsj['label'].shift(-3)

dff=[dfsj,dfiq]
df=pd.concat(dff)
 
df['high_season']=0
df['low_season']=0
df.loc[((df.weekofyear >=40) & (df.weekofyear<=45)),'high_season']=1
df.loc[((df.weekofyear >=10) & (df.weekofyear<=20)),'low_season']=1
  
df['outlier']=0
df.loc[((df.year==1994) & (df.weekofyear >=42) & (df.weekofyear<=49) & (df.city=='sj')) |((df.year==1998) & (df.weekofyear >=30) & (df.weekofyear<=36) & (df.city=='sj')),'outlier']=1
df=df.dropna()

"""# Removing corelated features"""

def corr_df(x, corr_val):
    '''
    Obj: Drops features that are strongly correlated to other features.
          This lowers model complexity, and aids in generalizing the model.
    Inputs:
          df: features df (x)
          corr_val: Columns are dropped relative to the corr_val input (e.g. 0.8)
    Output: df that only includes uncorrelated features
    '''


    corr_matrix = x.corr()
    iters = range(len(corr_matrix.columns) - 1)
    drop_cols = []

    # Iterates through Correlation Matrix Table to find correlated columns
    for i in iters:
        for j in range(i):
            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]
            col = item.columns
            row = item.index
            val = item.values
            if val >= corr_val:
                # Prints the correlated feature set and the corr val
                print(col.values[0], "|", row.values[0], "|", round(val[0][0], 2))
                drop_cols.append(i)
    drops = sorted(set(drop_cols))[::1]
     
# Drops the correlated columns
    for i in drops:
        col = x.iloc[:, (i+1):(i+2)].columns.values
        
        x = x.drop(col, axis=1)
     
    return x

print(df.shape[0]) 
print(df.shape[1])
df= corr_df(df,0.85)
print(df.shape[1])
df=df.drop(['week_start_date'],axis=1)
df=df.drop(['city'],axis=1)
print(df.shape[1])

"""# Spliting dataset 8:2 and MinMaxScaler features"""

train=df.sample(frac=0.8,random_state=0)
test=df.drop(train.index)
train_label=train.pop('label').values
test_label=test.pop('label').values

 
scaler = MinMaxScaler() # For normalizing dataset
X_train = scaler.fit_transform(train.as_matrix())

X_test = scaler.fit_transform(test.as_matrix())

"""# Build ANN and  Fit Model
1. Keras Sequence Model Neural Net
2. Neuron = (64,64,1)
3. Error Function = RMSE 
4. Regularization Parameter = l1(1e-4)
5. Drop Out rate = 50%
6. EarlyStopping = monitor='val_loss',patience=5
"""

model=Sequential()
model.add(Dense(64,input_dim=X_train.shape[1],activation='relu',activity_regularizer=regularizers.l1(1e-4)))
model.add(Dropout(0.5))
model.add(Dense(64,activation='relu',activity_regularizer=regularizers.l1(1e-4)))
model.add(Dropout(0.5))
model.add(Dense(1))
 
model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])

monitor=EarlyStopping(monitor='val_loss',patience=5,verbose=1,mode='auto')
checkpointer=ModelCheckpoint(filepath="best_weighs.hdfs",verbose=0,save_best_only=True)

model.fit(X_train,train_label,validation_data=(X_test,test_label),callbacks=[monitor,checkpointer],verbose=0,epochs=100, batch_size=15)
test_pred=model.predict(X_test)
train_pred=model.predict(X_train)

#score_training=metrics.mean_squared_error(pred,train_label)
score_test_MSE=metrics.mean_squared_error(test_pred,test_label)
score_train_MSE=metrics.mean_squared_error(train_pred,train_label)

print("Test RMSE: {:.4f}".format(np.sqrt(score_test_MSE)))
print("Training RMSE: {:.4f}".format(np.sqrt(score_train_MSE)))

def chart_regression(test_pred, y,sort=True):
    t=pd.DataFrame({'pred': test_pred, 'y': y.flatten()})
    #print(t)
    if sort:
        t.sort_values(by=['y'],inplace=True)
    plt.plot(t['y'].tolist(),label='actual')
    plt.plot(t['pred'].tolist(),label='predicted')
    plt.ylabel('output')
    plt.legend()
    plt.show()

chart_regression(test_pred.flatten(),test_label,False)

"""# Summary:
1. Just given features after removing correlation features, MSE=960
2. Add labels of previous week(pweek), MSE=139
3. Add pweek and seasonal feature, MSE=139
4. Add pweek and outlier feature, MSE=139
5. Add pweek, outlier and seasonal features, MSE = 115, 110 and 102 for three runs

Given training features plus adjusted pweek, outlier and seasonal features
6. Neuron(64/128/1), MSE=120
7. Neuron(64/64/1), MSE= see the step 5
8. Neuron(32/32/1), MSE=116
9. Neuron(32/32/32/1), MSE=225
10. Neuron(64/64/64/1), MSE=133

final results: 
Neuron = (64,64,1) 
Test RMSE: 10.9465
Training RMSE: 10.8073
"""